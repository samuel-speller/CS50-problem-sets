Times:

10 simulations: 0m0.027s (record time using 0m0.000s format)
100 simulations: 0m0.029s (record time using 0m0.000s format)
1000 simulations: 0m0.051s (record time using 0m0.000s format)
10000 simulations: 0m0.107s (record time using 0m0.000s format)
100000 simulations: 0m0.764s (record time using 0m0.000s format)
1000000 simulations: 0m7.843s (record time using 0m0.000s format)

Questions:

Which predictions, if any, proved incorrect as you increased the number of simulations?:

with N=10  the predictions were wildly off, with half the teams no even appearing in the predictions.
From N=1000 onwards the predictions seemed to settle down and even though the numbers changed slightly, the general gist stayed the same.

Suppose you're charged a fee for each second of compute time your program uses.
After how many simulations would you call the predictions "good enough"?:

The predictions stabalised around N = 10000, so I would stop there.